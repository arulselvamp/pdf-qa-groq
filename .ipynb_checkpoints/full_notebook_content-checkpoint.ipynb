{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "402aed31-b021-44fd-8c92-cad5101a19c4",
   "metadata": {},
   "source": [
    "pipeline: Load PDF → Split → Embed → Build FAISS → Save → Reload → RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa2635-7049-45bb-9a1a-57602fc611fa",
   "metadata": {},
   "source": [
    "# RetrievalQA pipeline (PDF → chunks → embeddings → FAISS → save/load → QA)\n",
    "\n",
    "**What this notebook does (end-to-end):**\n",
    "\n",
    "1. Load a local PDF (`sample.pdf`).\n",
    "2. Split into chunks.\n",
    "3. Create embeddings (Sentence-Transformers `all-MiniLM-L6-v2`).\n",
    "4. Build FAISS vectorstore and **save it locally**.\n",
    "5. Reload the FAISS index (demonstrates fast startup).\n",
    "6. Create a RetrievalQA chain using a remote Groq LLM wrapper (if available) and run queries.\n",
    "\n",
    "**Before running:**\n",
    "- Put a PDF named `sample.pdf` in the same folder as this notebook, or change the `PDF_PATH` variable below.\n",
    "- Make sure your environment has the required packages installed and the environment variables set (or use the inline `os.environ` cell to set them for the session):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb25bfb-2da1-4adf-91b0-92ec1bfa0275",
   "metadata": {},
   "source": [
    "\n",
    "- Recommended packages: `langchain`, `sentence-transformers`, `faiss-cpu`, `python-dotenv`, `requests`.\n",
    "\n",
    "This notebook is intended for development on a CPU-only machine and uses small embedding models to keep memory modest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0bbded6-749c-48a9-8c74-0257129c9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()   # reads .env from working directory\n",
    "\n",
    "if not os.getenv(\"GROQ_API_URL\"):\n",
    "    raise RuntimeError(\"GROQ_API_URL missing. Please create .env or set os.environ before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3ca6868-bf75-42a9-b1b4-cb56b26817b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip installation if packages already present.\n"
     ]
    }
   ],
   "source": [
    "# Optional: install required packages (uncomment to run)\n",
    "# !pip install langchain sentence-transformers faiss-cpu python-dotenv requests PyPDF2\n",
    "\n",
    "# If using Streamlit later:\n",
    "# !pip install streamlit\n",
    "\n",
    "print('Skip installation if packages already present.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42dd0b1-7e94-42e4-8ab7-d7616274acc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using groq_remote_llm.py from working directory\n",
      "GroqRemoteLLM available as class\n"
     ]
    }
   ],
   "source": [
    "# GroqRemoteLLM inline fallback (will prefer to import groq_remote_llm.py if present)\n",
    "try:\n",
    "    from groq_remote_llm import GroqRemoteLLM\n",
    "    print('Using groq_remote_llm.py from working directory')\n",
    "except Exception:\n",
    "    print('groq_remote_llm.py not found or failed to import — using inline fallback (works for basic calls)')\n",
    "    from langchain.llms.base import LLM\n",
    "    from typing import Optional, List, Mapping, Any\n",
    "    from pydantic import Field\n",
    "\n",
    "    class GroqRemoteLLM(LLM):\n",
    "        api_url: str = Field(default_factory=lambda: os.getenv('GROQ_API_URL'))\n",
    "        api_key: str = Field(default_factory=lambda: os.getenv('GROQ_API_KEY'))\n",
    "        model: str   = Field(default_factory=lambda: os.getenv('GROQ_MODEL', 'llama-3.3-70b-versatile'))\n",
    "        timeout: int = 60\n",
    "\n",
    "        @property\n",
    "        def _llm_type(self) -> str:\n",
    "            return 'groq-remote-llm'\n",
    "\n",
    "        @property\n",
    "        def _identifying_params(self) -> Mapping[str, Any]:\n",
    "            return {'model': self.model, 'url': self.api_url}\n",
    "\n",
    "        def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "            import requests\n",
    "            headers = {'Authorization': f'Bearer {self.api_key}', 'Content-Type': 'application/json'}\n",
    "            payload = {\n",
    "                'model': self.model,\n",
    "                'messages': [\n",
    "                    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "                    {'role': 'user', 'content': prompt},\n",
    "                ],\n",
    "                'max_tokens': 256,\n",
    "            }\n",
    "            resp = requests.post(self.api_url, json=payload, headers=headers, timeout=self.timeout)\n",
    "            resp.raise_for_status()\n",
    "            doc = resp.json()\n",
    "            try:\n",
    "                return doc['choices'][0]['message']['content']\n",
    "            except Exception:\n",
    "                return str(doc)\n",
    "\n",
    "print('GroqRemoteLLM available as class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54b382c-862b-459e-8484-ca674bcd9f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 pages\n",
      "Created 3 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arul\\AppData\\Local\\Temp\\ipykernel_4108\\2719709851.py:27: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 3 vectors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Config\n",
    "PDF_PATH = 'sample.pdf'  # change if needed\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n",
    "EMBED_MODEL = 'all-MiniLM-L6-v2'\n",
    "SAVE_DIR = 'faiss_index'\n",
    "\n",
    "# 1) Load PDF\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    raise FileNotFoundError(f\"Place a PDF named '{PDF_PATH}' in the notebook folder or update PDF_PATH\")\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "docs = loader.load()\n",
    "print('Loaded', len(docs), 'pages')\n",
    "\n",
    "# 2) Split into chunks\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print('Created', len(chunks), 'chunks')\n",
    "\n",
    "# 3) embeddings\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# 4) build FAISS vectorstore\n",
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "print('Vectorstore created with', len(chunks), 'vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a16fbf0-9c6b-41a5-9594-59c5965e6a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FAISS index to faiss_index\n"
     ]
    }
   ],
   "source": [
    "# Persist the FAISS index to disk\n",
    "vectorstore.save_local(SAVE_DIR)\n",
    "print('Saved FAISS index to', SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39e74db-7168-44cf-92b9-be3f061b814d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS index (pickle deserialization enabled). Retriever ready.\n"
     ]
    }
   ],
   "source": [
    "# reload_faiss_allow_pickle.py\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "SAVE_DIR = \"faiss_index\"\n",
    "\n",
    "emb = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# WARNING: allow_dangerous_deserialization=True will load pickled Python objects.\n",
    "# Only use this if you TRUST the files in SAVE_DIR (you created them locally).\n",
    "vectorstore = FAISS.load_local(SAVE_DIR, emb, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"Loaded FAISS index (pickle deserialization enabled). Retriever ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee364402-9aa4-4780-a962-a395c60cd6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUESTION: Summarize the main conclusion of the document in one sentence.\n",
      "Error while answering: NameError(\"name 'qa' is not defined\")\n",
      "\n",
      "QUESTION: What methods were used in the study?\n",
      "Error while answering: NameError(\"name 'qa' is not defined\")\n",
      "\n",
      "QUESTION: Who are the authors?\n",
      "Error while answering: NameError(\"name 'qa' is not defined\")\n"
     ]
    }
   ],
   "source": [
    "# Run a few queries and print answers\n",
    "queries = [\n",
    "    \"Summarize the main conclusion of the document in one sentence.\",\n",
    "    \"What methods were used in the study?\",\n",
    "    \"Who are the authors?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\nQUESTION:\", q)\n",
    "    try:\n",
    "        ans = qa.run(q)   # qa was created earlier as RetrievalQA\n",
    "        print(\"ANSWER:\", ans.strip())\n",
    "    except Exception as e:\n",
    "        print(\"Error while answering:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "924b9303-29bc-4ac5-82e0-5ff2d4eefbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrievalQA chain ready (qa). Now you can run qa.run(query).\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from groq_remote_llm import GroqRemoteLLM   # or use your inline GroqRemoteLLM fallback\n",
    "\n",
    "# Create Groq LLM\n",
    "llm = GroqRemoteLLM()\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",     # try \"map_reduce\" if doc is long\n",
    "    retriever=retriever     # retriever you already built from FAISS\n",
    ")\n",
    "\n",
    "print(\"RetrievalQA chain ready (qa). Now you can run qa.run(query).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00b7643d-981f-4d49-a0fd-b829b0b120bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ_API_URL: 'https://api.groq.com/openai/v1/chat/completions'\n",
      "GROQ_API_KEY: True\n",
      "GROQ_MODEL: 'llama-3.3-70b-versatile'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load .env if present\n",
    "load_dotenv()\n",
    "\n",
    "print(\"GROQ_API_URL:\", repr(os.getenv(\"GROQ_API_URL\")))\n",
    "print(\"GROQ_API_KEY:\", bool(os.getenv(\"GROQ_API_KEY\")))   # prints True if key present (keeps it hidden)\n",
    "print(\"GROQ_MODEL:\", repr(os.getenv(\"GROQ_MODEL\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e25a5cc-d719-4aea-9187-9d0191306525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP 200\n",
      "{\n",
      "  \"id\": \"chatcmpl-06224f50-2d76-4568-815a-1707623925a9\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1759307089,\n",
      "  \"model\": \"llama-3.3-70b-versatile\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"HELLO\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"queue_time\": 0.050131119,\n",
      "    \"prompt_tokens\": 44,\n",
      "    \"prompt_time\": 0.002049261,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"completion_time\": 0.009642606,\n",
      "    \"total_tokens\": 47,\n",
      "    \"total_time\": 0.011691867\n",
      "  },\n",
      "  \"usage_breakdown\": null,\n",
      "  \"system_fingerprint\": \"fp_155ab82e98\",\n",
      "  \"x_groq\": {\n",
      "    \"id\": \"req_01k6fdzhe4fjqv8vfad2kt38yd\"\n",
      "  },\n",
      "  \"service_tier\": \"on_demand\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, os, json\n",
    "API_URL = os.getenv(\"GROQ_API_URL\")\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "MODEL = os.getenv(\"GROQ_MODEL\")\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"model\": MODEL,\n",
    "    \"messages\": [\n",
    "        {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
    "        {\"role\":\"user\",\"content\":\"Say HELLO\"}\n",
    "    ],\n",
    "    \"max_tokens\": 20\n",
    "}\n",
    "\n",
    "resp = requests.post(API_URL, json=payload, headers=headers, timeout=20)\n",
    "print(\"HTTP\", resp.status_code)\n",
    "try:\n",
    "    print(json.dumps(resp.json(), indent=2))\n",
    "except Exception:\n",
    "    print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "378ff2cb-d0c1-46b0-9c76-de4db15166b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM api_url: https://api.groq.com/openai/v1/chat/completions\n",
      "{'query': 'What is the main conclusion of the document?', 'result': 'Not stated in the document. \\n\\nThe document appears to be a collection of abstracts or summaries of various research papers presented at the 15th International Conference on Science and Innovative Engineering 2025, and does not have a single main conclusion. Each paper has its own conclusion or contribution, but there is no overarching conclusion for the entire document.'}\n"
     ]
    }
   ],
   "source": [
    "# reload module if updated on disk\n",
    "import importlib, sys\n",
    "if \"groq_remote_llm\" in sys.modules:\n",
    "    importlib.reload(sys.modules[\"groq_remote_llm\"])\n",
    "\n",
    "from groq_remote_llm import GroqRemoteLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = GroqRemoteLLM()\n",
    "print(\"LLM api_url:\", llm.api_url)\n",
    "\n",
    "# recreate qa (assumes retriever exists)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "# Use invoke to run (some versions expect dict input)\n",
    "# For RetrievalQA chain, pass {\"query\": \"...\"} or just use qa.run if you prefer\n",
    "try:\n",
    "    # prefer qa.invoke to satisfy new API (returns a dict)\n",
    "    out = qa.invoke({\"query\": \"What is the main conclusion of the document?\"})\n",
    "    # The output key is usually \"output_text\" or similar; print whole dict:\n",
    "    print(out)\n",
    "except Exception as e:\n",
    "    # fallback to .run for quick test (older style)\n",
    "    print(\"invoke failed, trying run():\", e)\n",
    "    print(\"run() output:\", qa.run(\"What is the main conclusion of the document?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af82756e-c85b-46fd-9055-f7aeffa99b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP status: 200\n",
      "Response (first 2000 chars):\n",
      "{\"id\":\"chatcmpl-f9561049-a860-48a3-8c6e-4c4cecc1e023\",\"object\":\"chat.completion\",\"created\":1759307103,\"model\":\"llama-3.3-70b-versatile\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"HELLO\"},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"usage\":{\"queue_time\":0.048795471,\"prompt_tokens\":44,\"prompt_time\":0.002932659,\"completion_tokens\":3,\"completion_time\":0.008652851,\"total_tokens\":47,\"total_time\":0.01158551},\"usage_breakdown\":null,\"system_fingerprint\":\"fp_155ab82e98\",\"x_groq\":{\"id\":\"req_01k6fdzzkmfwfb8pp22egs4b5w\"},\"service_tier\":\"on_demand\"}\n",
      "\n",
      "\\nExtracted assistant content: 'HELLO'\n"
     ]
    }
   ],
   "source": [
    "import requests, os, json\n",
    "API_URL = os.getenv(\"GROQ_API_URL\")\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "MODEL = os.getenv(\"GROQ_MODEL\")\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"model\": MODEL,\n",
    "    \"messages\": [\n",
    "        {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
    "        {\"role\":\"user\",\"content\":\"Say HELLO\"}\n",
    "    ],\n",
    "    \"max_tokens\": 20\n",
    "}\n",
    "\n",
    "try:\n",
    "    resp = requests.post(API_URL, json=payload, headers=headers, timeout=20)\n",
    "except Exception as e:\n",
    "    print(\"Request failed:\", repr(e))\n",
    "else:\n",
    "    print(\"HTTP status:\", resp.status_code)\n",
    "    txt = resp.text[:2000]  # show up to first 2000 chars\n",
    "    print(\"Response (first 2000 chars):\")\n",
    "    print(txt)\n",
    "    # try to extract assistant text if possible\n",
    "    try:\n",
    "        doc = resp.json()\n",
    "        content = None\n",
    "        if isinstance(doc, dict):\n",
    "            if \"choices\" in doc and isinstance(doc[\"choices\"], list) and doc[\"choices\"]:\n",
    "                c = doc[\"choices\"][0]\n",
    "                # OpenAI style: c['message']['content']\n",
    "                if isinstance(c, dict):\n",
    "                    if \"message\" in c and isinstance(c[\"message\"], dict) and \"content\" in c[\"message\"]:\n",
    "                        content = c[\"message\"][\"content\"]\n",
    "                    elif \"text\" in c:\n",
    "                        content = c[\"text\"]\n",
    "            elif \"output\" in doc:\n",
    "                content = doc[\"output\"]\n",
    "            elif \"result\" in doc:\n",
    "                content = doc[\"result\"]\n",
    "        if content:\n",
    "            print(\"\\\\nExtracted assistant content:\", repr(content))\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c72fec-ebbb-44ac-a5bc-1658b4cdb807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM api_url: https://api.groq.com/openai/v1/chat/completions\n",
      "RetrievalQA ready.\n"
     ]
    }
   ],
   "source": [
    "# reload module in case you edited it\n",
    "import importlib, sys\n",
    "if \"groq_remote_llm\" in sys.modules:\n",
    "    importlib.reload(sys.modules[\"groq_remote_llm\"])\n",
    "\n",
    "from groq_remote_llm import GroqRemoteLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = GroqRemoteLLM()   # should show api_url when constructed if you added the check\n",
    "print(\"LLM api_url:\", llm.api_url)\n",
    "\n",
    "# recreate the QA chain (assumes `retriever` exists)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "print(\"RetrievalQA ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9fbedf5-982e-4bd5-bfbd-016827cc5cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw result keys: ['query', 'result']\n",
      "{\n",
      "  \"query\": \"What is the main conclusion of the document?\",\n",
      "  \"result\": \"Not stated in the document. \\n\\nThe document appears to be a collection of abstracts or summaries of various research papers presented at the 15th International Conference on Science and Innovative Engineering 2025, and it does not provide a main conclusion. Each section discusses a different topic, and there is no overarching conclusion that ties the entire document together.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main conclusion of the document?\"\n",
    "\n",
    "# invoke returns a dict-like result with varying keys depending on LangChain version\n",
    "result = qa.invoke({\"query\": query})\n",
    "print(\"Raw result keys:\", list(result.keys()))\n",
    "# pretty-print the returned dict (safe small view)\n",
    "import json\n",
    "print(json.dumps(result, indent=2)[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "764e93fa-af57-4bb4-a554-2b28b875e5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " Not stated in the document. \n",
      "\n",
      "The document appears to be a collection of abstracts or summaries of various research papers presented at the 15th International Conference on Science and Innovative Engineering 2025, and it does not provide a main conclusion. Each section discusses a different topic, and there is no overarching conclusion that ties the entire document together.\n",
      "\n",
      "No source_documents returned with the result. You can still fetch them via retriever.get_relevant_documents(query).\n"
     ]
    }
   ],
   "source": [
    "# Robust extraction\n",
    "answer = None\n",
    "\n",
    "# common keys to check\n",
    "for key in (\"output_text\", \"result\", \"answer\", \"text\", \"output\"):\n",
    "    if key in result:\n",
    "        answer = result[key]\n",
    "        break\n",
    "\n",
    "# some versions embed the text under the chain's output key\n",
    "if not answer:\n",
    "    # print everything and then choose a likely field\n",
    "    # try the first string value\n",
    "    for v in result.values():\n",
    "        if isinstance(v, str) and len(v) > 0:\n",
    "            answer = v\n",
    "            break\n",
    "\n",
    "print(\"ANSWER:\\n\", answer)\n",
    "\n",
    "# If the chain returned source documents (sometimes under 'source_documents' or similar), print them\n",
    "if \"source_documents\" in result:\n",
    "    docs = result[\"source_documents\"]\n",
    "elif \"source_docs\" in result:\n",
    "    docs = result[\"source_docs\"]\n",
    "else:\n",
    "    # try to fetch returned sources from qa if available\n",
    "    try:\n",
    "        # Some RetrievalQA implementations support return_source_documents flag — might be in 'result'\n",
    "        docs = result.get(\"source_documents\", None)\n",
    "    except Exception:\n",
    "        docs = None\n",
    "\n",
    "if docs:\n",
    "    print(f\"\\nRetrieved {len(docs)} source documents:\")\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        src = getattr(d, \"metadata\", {}).get(\"source\", \"unknown\")\n",
    "        print(f\"\\n--- SOURCE {i} (source: {src}) ---\")\n",
    "        print(d.page_content[:800])\n",
    "else:\n",
    "    print(\"\\nNo source_documents returned with the result. You can still fetch them via retriever.get_relevant_documents(query).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe8513b-9650-4b0e-8917-cc56e51aaff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arul\\AppData\\Local\\Temp\\ipykernel_4108\\282996325.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  out = qa.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa.run output:\n",
      " Not stated in the document. The document appears to be a collection of abstracts or summaries of various research papers presented at a conference, and it does not have a single main conclusion. Each paper has its own conclusion and findings, but there is no overarching conclusion that ties the entire document together.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    out = qa.run(query)\n",
    "    print(\"qa.run output:\\n\", out)\n",
    "except Exception as e:\n",
    "    print(\"qa.run failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba8c4f66-dc67-4f92-bdbf-2c375f306657",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "res = qa.invoke({\"query\": query})\n",
    "# then extract res['source_documents'] as above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d60b861-3948-4d8f-9c39-22056ad6cf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pages: 3\n",
      "Created chunks: 3\n",
      "Saved chunks JSONL to: faiss_index\\chunks.jsonl\n",
      "Built FAISS vectorstore with vectors: 3\n",
      "Saved FAISS index to folder: faiss_index\n",
      "Retriever ready with k=2. Done.\n"
     ]
    }
   ],
   "source": [
    "# === REBUILD FAISS WITH SMALLER CHUNKS (Notebook cell) ===\n",
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Config - change PDF_PATH if needed\n",
    "PDF_PATH = \"sample.pdf\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 80\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "SAVE_DIR = \"faiss_index\"\n",
    "CHUNKS_JSONL = os.path.join(SAVE_DIR, \"chunks.jsonl\")\n",
    "\n",
    "# Imports (make sure packages are installed)\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 1) Load PDF\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    raise FileNotFoundError(f\"Put your PDF at '{PDF_PATH}' or change the PDF_PATH variable.\")\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "docs = loader.load()\n",
    "print(\"Loaded pages:\", len(docs))\n",
    "\n",
    "# 2) Split into smaller chunks\n",
    "splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(\"Created chunks:\", len(chunks))\n",
    "\n",
    "# 3) Save chunks to JSONL (safe, portable)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "with open(CHUNKS_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for doc in chunks:\n",
    "        rec = {\"page_content\": doc.page_content, \"metadata\": getattr(doc, \"metadata\", {})}\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "print(\"Saved chunks JSONL to:\", CHUNKS_JSONL)\n",
    "\n",
    "# 4) Create embeddings and FAISS vectorstore\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "print(\"Built FAISS vectorstore with vectors:\", len(chunks))\n",
    "\n",
    "# 5) Persist FAISS index to disk\n",
    "vectorstore.save_local(SAVE_DIR)\n",
    "print(\"Saved FAISS index to folder:\", SAVE_DIR)\n",
    "\n",
    "# 6) Create retriever for immediate use (k=2 recommended)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "print(\"Retriever ready with k=2. Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20166819-6f08-4ba2-89e7-3c46cca9e269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw keys: ['query', 'result', 'source_documents']\n",
      "\n",
      "=== MAP-REDUCE ANSWER ===\n",
      " The methodology section of the paper describes the use of a GRU-CNN hybrid ML approach to categorize assaults. The model is trained using pre-existing static patterns and then used to categorize insider attacks using various case studies, with the Insider Threat Test Dataset used to assess its effectiveness. The proposed Hybrid Classification Strategy (HCS) showed high accuracy and low false positive rates in both training and testing phases.\n",
      "\n",
      "--- SOURCE 1 ---\n",
      "Proceedings of 15th International Conference on Science and Innovative Engineering 2025 \n",
      "April 26th - 27th, 2025 \n",
      "Prince Dr.K.Vasudevan college of Engineering and Technology, India  \n",
      "Manipal University College Malaysia, Melaka, Malaysia           ISBN 978-81-983498-5-9                                                                                                                       \n",
      " \n",
      " \n",
      "117. LMS PLATFORM USING GENERATIVE AI \n",
      " \n",
      "Ruben George Varghese \n",
      "Dharshan R E \n",
      "Harish Jayaram S S \n",
      "R Dheepthi \n",
      "Computer Science and Engineering \n",
      "Hindustan Institute of Technology and Science, \n",
      "Chennai, Tamil Nadu, India \n",
      " \n",
      "The \"LMS Platform Using Generative AI\" addresses the lack of personalized and engaging learning \n",
      "resources in traditional Learning Management Systems (LMS). By overcoming the limitation\n",
      "\n",
      "--- SOURCE 2 ---\n",
      "Proceedings of 15th International Conference on Science and Innovative Engineering 2025 \n",
      "April 26th - 27th, 2025 \n",
      "Prince Dr.K.Vasudevan college of Engineering and Technology, India  \n",
      "Manipal University College Malaysia, Melaka, Malaysia           ISBN 978-81-983498-5-9                                                                                                                       \n",
      " \n",
      " \n",
      " \n",
      "Sidon sets-SSs are subsets of real numbers possessing different totals for pair wise sums. Simon Sidon \n",
      "introduced it to settle a problem in harmonic analysis. Sidon, Erdos and Turan formed a group of three \n",
      "to popularize its study in 1934. In the field of combinatorics, the SSs are hotly pursued objects as these \n",
      "are employed in graph theory, coding theory, distributed computing etc. The task of deter\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from groq_remote_llm import GroqRemoteLLM\n",
    "\n",
    "# Create LLM instance (ensure env vars loaded)\n",
    "llm = GroqRemoteLLM()\n",
    "\n",
    "# Build a map-reduce RetrievalQA\n",
    "qa_map = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",   # map_reduce does per-chunk summaries then reduces\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True    # ask it to return sources too\n",
    ")\n",
    "\n",
    "query = \"Summarize the methodology section of the paper.\"\n",
    "res = qa_map.invoke({\"query\": query})\n",
    "print(\"Raw keys:\", list(res.keys()))\n",
    "# Extract answer robustly\n",
    "answer = res.get(\"output_text\") or res.get(\"result\") or next((v for v in res.values() if isinstance(v, str)), None)\n",
    "print(\"\\n=== MAP-REDUCE ANSWER ===\\n\", answer)\n",
    "\n",
    "# If sources returned, print them\n",
    "docs = res.get(\"source_documents\") or res.get(\"source_docs\")\n",
    "if docs:\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        print(f\"\\n--- SOURCE {i} ---\\n{d.page_content[:800]}\")\n",
    "else:\n",
    "    print(\"\\nNo source_documents in response — use retriever.get_relevant_documents(query) to inspect sources.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aec364c5-8d43-46be-a8bf-a8d88cea000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Not stated in the document.\n",
      "\n",
      "--- SOURCE 1 ---\n",
      "Proceedings of 15th International Conference on Science and Innovative Engineering 2025 \n",
      "April 26th - 27th, 2025 \n",
      "Prince Dr.K.Vasudevan college of Engineering and Technology, India  \n",
      "Manipal University College Malaysia, Melaka, Malaysia           ISBN 978-81-983498-5-9                                                                                                                       \n",
      " \n",
      " \n",
      " \n",
      "Sidon sets-SSs are subsets of real numbers possessing different totals for pair wise sums. Simon Sidon \n",
      "i\n",
      "\n",
      "--- SOURCE 2 ---\n",
      "Proceedings of 15th International Conference on Science and Innovative Engineering 2025 \n",
      "April 26th - 27th, 2025 \n",
      "Prince Dr.K.Vasudevan college of Engineering and Technology, India  \n",
      "Manipal University College Malaysia, Melaka, Malaysia           ISBN 978-81-983498-5-9                                                                                                                       \n",
      " \n",
      " \n",
      "Jerusalem College of Engineering, Chennai-600100. \n",
      " \n",
      "In the era of big data, cloud storage services have be\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from groq_remote_llm import GroqRemoteLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "emb = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", emb, allow_dangerous_deserialization=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})  # k=2 for precision\n",
    "\n",
    "llm = GroqRemoteLLM()\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "query = \"What is the main conclusion of the paper?\"\n",
    "res = qa.invoke({\"query\": query})\n",
    "print(\"Answer:\", res.get(\"output_text\") or res.get(\"result\"))\n",
    "\n",
    "# Inspect sources\n",
    "for i, d in enumerate(res.get(\"source_documents\", []), 1):\n",
    "    print(f\"\\n--- SOURCE {i} ---\\n{d.page_content[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "051187bd-024c-49ee-8016-7fbdfd3c17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from groq_remote_llm import GroqRemoteLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "emb = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", emb, allow_dangerous_deserialization=True)  # only if you saved via save_local\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "llm = GroqRemoteLLM()\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=retriever, return_source_documents=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b643bb6-45db-4b5d-9b04-8b6c4635a7d9",
   "metadata": {},
   "source": [
    "**Notes & troubleshooting**\n",
    "\n",
    "- If the Groq LLM call fails, check your `.env` and run the connectivity `requests.post` test shown earlier.\n",
    "- If `SentenceTransformer` import errors occur, set `os.environ['TRANSFORMERS_NO_TF']='1'` before importing or use a clean conda env.\n",
    "- To reuse the saved FAISS index in a Streamlit app, use `FAISS.load_local` at app startup.\n",
    "\n",
    "Enjoy — run the cells sequentially from top to bottom. If you want, I can also provide this notebook as a downloadable file.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
