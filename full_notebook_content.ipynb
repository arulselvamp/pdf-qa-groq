{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "402aed31-b021-44fd-8c92-cad5101a19c4",
   "metadata": {},
   "source": [
    "pipeline: Load PDF → Split → Embed → Build FAISS → Save → Reload → RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa2635-7049-45bb-9a1a-57602fc611fa",
   "metadata": {},
   "source": [
    "# RetrievalQA pipeline (PDF → chunks → embeddings → FAISS → save/load → QA)\n",
    "\n",
    "**What this notebook does (end-to-end):**\n",
    "\n",
    "1. Load a local PDF (`sample.pdf`).\n",
    "2. Split into chunks.\n",
    "3. Create embeddings (Sentence-Transformers `all-MiniLM-L6-v2`).\n",
    "4. Build FAISS vectorstore and **save it locally**.\n",
    "5. Reload the FAISS index (demonstrates fast startup).\n",
    "6. Create a RetrievalQA chain using a remote Groq LLM wrapper (if available) and run queries.\n",
    "\n",
    "**Before running:**\n",
    "- Put a PDF named `sample.pdf` in the same folder as this notebook, or change the `PDF_PATH` variable below.\n",
    "- Make sure your environment has the required packages installed and the environment variables set (or use the inline `os.environ` cell to set them for the session):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb25bfb-2da1-4adf-91b0-92ec1bfa0275",
   "metadata": {},
   "source": [
    "\n",
    "- Recommended packages: `langchain`, `sentence-transformers`, `faiss-cpu`, `python-dotenv`, `requests`.\n",
    "\n",
    "This notebook is intended for development on a CPU-only machine and uses small embedding models to keep memory modest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bbded6-749c-48a9-8c74-0257129c9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()   # reads .env from working directory\n",
    "\n",
    "if not os.getenv(\"GROQ_API_URL\"):\n",
    "    raise RuntimeError(\"GROQ_API_URL missing. Please create .env or set os.environ before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ca6868-bf75-42a9-b1b4-cb56b26817b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip installation if packages already present.\n"
     ]
    }
   ],
   "source": [
    "# Optional: install required packages (uncomment to run)\n",
    "# !pip install langchain sentence-transformers faiss-cpu python-dotenv requests PyPDF2\n",
    "\n",
    "# If using Streamlit later:\n",
    "# !pip install streamlit\n",
    "\n",
    "print('Skip installation if packages already present.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a42dd0b1-7e94-42e4-8ab7-d7616274acc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using groq_remote_llm.py from working directory\n",
      "GroqRemoteLLM available as class\n"
     ]
    }
   ],
   "source": [
    "# GroqRemoteLLM inline fallback (will prefer to import groq_remote_llm.py if present)\n",
    "try:\n",
    "    from groq_remote_llm import GroqRemoteLLM\n",
    "    print('Using groq_remote_llm.py from working directory')\n",
    "except Exception:\n",
    "    print('groq_remote_llm.py not found or failed to import — using inline fallback (works for basic calls)')\n",
    "    from langchain.llms.base import LLM\n",
    "    from typing import Optional, List, Mapping, Any\n",
    "    from pydantic import Field\n",
    "\n",
    "    class GroqRemoteLLM(LLM):\n",
    "        api_url: str = Field(default_factory=lambda: os.getenv('GROQ_API_URL'))\n",
    "        api_key: str = Field(default_factory=lambda: os.getenv('GROQ_API_KEY'))\n",
    "        model: str   = Field(default_factory=lambda: os.getenv('GROQ_MODEL', 'llama-3.3-70b-versatile'))\n",
    "        timeout: int = 60\n",
    "\n",
    "        @property\n",
    "        def _llm_type(self) -> str:\n",
    "            return 'groq-remote-llm'\n",
    "\n",
    "        @property\n",
    "        def _identifying_params(self) -> Mapping[str, Any]:\n",
    "            return {'model': self.model, 'url': self.api_url}\n",
    "\n",
    "        def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "            import requests\n",
    "            headers = {'Authorization': f'Bearer {self.api_key}', 'Content-Type': 'application/json'}\n",
    "            payload = {\n",
    "                'model': self.model,\n",
    "                'messages': [\n",
    "                    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "                    {'role': 'user', 'content': prompt},\n",
    "                ],\n",
    "                'max_tokens': 256,\n",
    "            }\n",
    "            resp = requests.post(self.api_url, json=payload, headers=headers, timeout=self.timeout)\n",
    "            resp.raise_for_status()\n",
    "            doc = resp.json()\n",
    "            try:\n",
    "                return doc['choices'][0]['message']['content']\n",
    "            except Exception:\n",
    "                return str(doc)\n",
    "\n",
    "print('GroqRemoteLLM available as class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54b382c-862b-459e-8484-ca674bcd9f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 pages\n",
      "Created 3 chunks\n",
      "Vectorstore created with 3 vectors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Config\n",
    "PDF_PATH = 'sample.pdf'  # change if needed\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n",
    "EMBED_MODEL = 'all-MiniLM-L6-v2'\n",
    "SAVE_DIR = 'faiss_index'\n",
    "\n",
    "# 1) Load PDF\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    raise FileNotFoundError(f\"Place a PDF named '{PDF_PATH}' in the notebook folder or update PDF_PATH\")\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "docs = loader.load()\n",
    "print('Loaded', len(docs), 'pages')\n",
    "\n",
    "# 2) Split into chunks\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print('Created', len(chunks), 'chunks')\n",
    "\n",
    "# 3) embeddings\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# 4) build FAISS vectorstore\n",
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "print('Vectorstore created with', len(chunks), 'vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a16fbf0-9c6b-41a5-9594-59c5965e6a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FAISS index to faiss_index\n"
     ]
    }
   ],
   "source": [
    "# Persist the FAISS index to disk\n",
    "vectorstore.save_local(SAVE_DIR)\n",
    "print('Saved FAISS index to', SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39e74db-7168-44cf-92b9-be3f061b814d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS index (pickle deserialization enabled). Retriever ready.\n"
     ]
    }
   ],
   "source": [
    "# reload_faiss_allow_pickle.py\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "SAVE_DIR = \"faiss_index\"\n",
    "\n",
    "emb = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# WARNING: allow_dangerous_deserialization=True will load pickled Python objects.\n",
    "# Only use this if you TRUST the files in SAVE_DIR (you created them locally).\n",
    "vectorstore = FAISS.load_local(SAVE_DIR, emb, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"Loaded FAISS index (pickle deserialization enabled). Retriever ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee364402-9aa4-4780-a962-a395c60cd6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUESTION: Summarize the main conclusion of the document in one sentence.\n",
      "Error while answering: ValueError(\"`run` not supported when there is not exactly one output key. Got ['result', 'source_documents'].\")\n",
      "\n",
      "QUESTION: What methods were used in the study?\n",
      "Error while answering: ValueError(\"`run` not supported when there is not exactly one output key. Got ['result', 'source_documents'].\")\n",
      "\n",
      "QUESTION: Who are the authors?\n",
      "Error while answering: ValueError(\"`run` not supported when there is not exactly one output key. Got ['result', 'source_documents'].\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arul\\AppData\\Local\\Temp\\ipykernel_2588\\3600311847.py:11: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  ans = qa.run(q)   # qa was created earlier as RetrievalQA\n"
     ]
    }
   ],
   "source": [
    "# Run a few queries and print answers\n",
    "queries = [\n",
    "    \"Summarize the main conclusion of the document in one sentence.\",\n",
    "    \"What methods were used in the study?\",\n",
    "    \"Who are the authors?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\nQUESTION:\", q)\n",
    "    try:\n",
    "        ans = qa.run(q)   # qa was created earlier as RetrievalQA\n",
    "        print(\"ANSWER:\", ans.strip())\n",
    "    except Exception as e:\n",
    "        print(\"Error while answering:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "924b9303-29bc-4ac5-82e0-5ff2d4eefbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrievalQA chain ready (qa). Now you can run qa.run(query).\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from groq_remote_llm import GroqRemoteLLM   # or use your inline GroqRemoteLLM fallback\n",
    "\n",
    "# Create Groq LLM\n",
    "llm = GroqRemoteLLM()\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",     # try \"map_reduce\" if doc is long\n",
    "    retriever=retriever     # retriever you already built from FAISS\n",
    ")\n",
    "\n",
    "print(\"RetrievalQA chain ready (qa). Now you can run qa.run(query).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00b7643d-981f-4d49-a0fd-b829b0b120bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ_API_URL: 'https://api.groq.com/openai/v1/chat/completions'\n",
      "GROQ_API_KEY: True\n",
      "GROQ_MODEL: 'llama-3.3-70b-versatile'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load .env if present\n",
    "load_dotenv()\n",
    "\n",
    "print(\"GROQ_API_URL:\", repr(os.getenv(\"GROQ_API_URL\")))\n",
    "print(\"GROQ_API_KEY:\", bool(os.getenv(\"GROQ_API_KEY\")))   # prints True if key present (keeps it hidden)\n",
    "print(\"GROQ_MODEL:\", repr(os.getenv(\"GROQ_MODEL\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e25a5cc-d719-4aea-9187-9d0191306525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP 200\n",
      "{\n",
      "  \"id\": \"chatcmpl-2924523e-1a50-429e-9a91-6c9182c1c55d\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1759416019,\n",
      "  \"model\": \"llama-3.3-70b-versatile\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"HELLO\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"queue_time\": 0.049026797,\n",
      "    \"prompt_tokens\": 44,\n",
      "    \"prompt_time\": 0.001960063,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"completion_time\": 0.00963365,\n",
      "    \"total_tokens\": 47,\n",
      "    \"total_time\": 0.011593713\n",
      "  },\n",
      "  \"usage_breakdown\": null,\n",
      "  \"system_fingerprint\": \"fp_9e1e8f8435\",\n",
      "  \"x_groq\": {\n",
      "    \"id\": \"req_01k6jnvtn6f23befn5pdwaj7pg\"\n",
      "  },\n",
      "  \"service_tier\": \"on_demand\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, os, json\n",
    "API_URL = os.getenv(\"GROQ_API_URL\")\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "MODEL = os.getenv(\"GROQ_MODEL\")\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"model\": MODEL,\n",
    "    \"messages\": [\n",
    "        {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
    "        {\"role\":\"user\",\"content\":\"Say HELLO\"}\n",
    "    ],\n",
    "    \"max_tokens\": 20\n",
    "}\n",
    "\n",
    "resp = requests.post(API_URL, json=payload, headers=headers, timeout=20)\n",
    "print(\"HTTP\", resp.status_code)\n",
    "try:\n",
    "    print(json.dumps(resp.json(), indent=2))\n",
    "except Exception:\n",
    "    print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378ff2cb-d0c1-46b0-9c76-de4db15166b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM api_url: https://api.groq.com/openai/v1/chat/completions\n",
      "{'query': 'What is the main conclusion of the document?', 'result': 'The document does not present a single main conclusion, as it appears to be a collection of abstracts or summaries of various research papers and projects presented at the 15th International Conference on Science and Innovative Engineering 2025. Each section describes a different topic, such as data deduplication, grievance resolution, graphs based on Sidon sets, and smart wheelchairs, among others. Therefore, there is no overarching conclusion that ties the entire document together.'}\n"
     ]
    }
   ],
   "source": [
    "# reload module if updated on disk\n",
    "import importlib, sys\n",
    "if \"groq_remote_llm\" in sys.modules:\n",
    "    importlib.reload(sys.modules[\"groq_remote_llm\"])\n",
    "\n",
    "from groq_remote_llm import GroqRemoteLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = GroqRemoteLLM()\n",
    "print(\"LLM api_url:\", llm.api_url)\n",
    "\n",
    "# recreate qa (assumes retriever exists)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "# Use invoke to run (some versions expect dict input)\n",
    "# For RetrievalQA chain, pass {\"query\": \"...\"} or just use qa.run if you prefer\n",
    "try:\n",
    "    # prefer qa.invoke to satisfy new API (returns a dict)\n",
    "    out = qa.invoke({\"query\": \"What is the main conclusion of the document?\"})\n",
    "    # The output key is usually \"output_text\" or similar; print whole dict:\n",
    "    print(out)\n",
    "except Exception as e:\n",
    "    # fallback to .run for quick test (older style)\n",
    "    print(\"invoke failed, trying run():\", e)\n",
    "    print(\"run() output:\", qa.run(\"What is the main conclusion of the document?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af82756e-c85b-46fd-9055-f7aeffa99b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP status: 200\n",
      "Response (first 2000 chars):\n",
      "{\"id\":\"chatcmpl-8d52ec5e-beb1-4762-9763-fa16bf4209db\",\"object\":\"chat.completion\",\"created\":1759416022,\"model\":\"llama-3.3-70b-versatile\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"HELLO\"},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"usage\":{\"queue_time\":0.048215622,\"prompt_tokens\":44,\"prompt_time\":0.002205348,\"completion_tokens\":3,\"completion_time\":0.009568862,\"total_tokens\":47,\"total_time\":0.01177421},\"usage_breakdown\":null,\"system_fingerprint\":\"fp_9e1e8f8435\",\"x_groq\":{\"id\":\"req_01k6jnvwyff25bvqhvc558qt3b\"},\"service_tier\":\"on_demand\"}\n",
      "\n",
      "\\nExtracted assistant content: 'HELLO'\n"
     ]
    }
   ],
   "source": [
    "import requests, os, json\n",
    "API_URL = os.getenv(\"GROQ_API_URL\")\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "MODEL = os.getenv(\"GROQ_MODEL\")\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"model\": MODEL,\n",
    "    \"messages\": [\n",
    "        {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
    "        {\"role\":\"user\",\"content\":\"Say HELLO\"}\n",
    "    ],\n",
    "    \"max_tokens\": 20\n",
    "}\n",
    "\n",
    "try:\n",
    "    resp = requests.post(API_URL, json=payload, headers=headers, timeout=20)\n",
    "except Exception as e:\n",
    "    print(\"Request failed:\", repr(e))\n",
    "else:\n",
    "    print(\"HTTP status:\", resp.status_code)\n",
    "    txt = resp.text[:2000]  # show up to first 2000 chars\n",
    "    print(\"Response (first 2000 chars):\")\n",
    "    print(txt)\n",
    "    # try to extract assistant text if possible\n",
    "    try:\n",
    "        doc = resp.json()\n",
    "        content = None\n",
    "        if isinstance(doc, dict):\n",
    "            if \"choices\" in doc and isinstance(doc[\"choices\"], list) and doc[\"choices\"]:\n",
    "                c = doc[\"choices\"][0]\n",
    "                # OpenAI style: c['message']['content']\n",
    "                if isinstance(c, dict):\n",
    "                    if \"message\" in c and isinstance(c[\"message\"], dict) and \"content\" in c[\"message\"]:\n",
    "                        content = c[\"message\"][\"content\"]\n",
    "                    elif \"text\" in c:\n",
    "                        content = c[\"text\"]\n",
    "            elif \"output\" in doc:\n",
    "                content = doc[\"output\"]\n",
    "            elif \"result\" in doc:\n",
    "                content = doc[\"result\"]\n",
    "        if content:\n",
    "            print(\"\\\\nExtracted assistant content:\", repr(content))\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c72fec-ebbb-44ac-a5bc-1658b4cdb807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM api_url: https://api.groq.com/openai/v1/chat/completions\n",
      "RetrievalQA ready.\n"
     ]
    }
   ],
   "source": [
    "# reload module in case you edited it\n",
    "import importlib, sys\n",
    "if \"groq_remote_llm\" in sys.modules:\n",
    "    importlib.reload(sys.modules[\"groq_remote_llm\"])\n",
    "\n",
    "from groq_remote_llm import GroqRemoteLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = GroqRemoteLLM()   # should show api_url when constructed if you added the check\n",
    "print(\"LLM api_url:\", llm.api_url)\n",
    "\n",
    "# recreate the QA chain (assumes `retriever` exists)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "print(\"RetrievalQA ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9fbedf5-982e-4bd5-bfbd-016827cc5cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw result keys: ['query', 'result']\n",
      "{\n",
      "  \"query\": \"What is the main conclusion of the document?\",\n",
      "  \"result\": \"The document appears to be a collection of proceedings from the 15th International Conference on Science and Innovative Engineering 2025, featuring various research papers on different topics. As such, there is no single main conclusion that can be drawn from the document as a whole. Each paper presents its own findings and conclusions, but there is no overarching conclusion that ties the entire document together.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main conclusion of the document?\"\n",
    "\n",
    "# invoke returns a dict-like result with varying keys depending on LangChain version\n",
    "result = qa.invoke({\"query\": query})\n",
    "print(\"Raw result keys:\", list(result.keys()))\n",
    "# pretty-print the returned dict (safe small view)\n",
    "import json\n",
    "print(json.dumps(result, indent=2)[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "764e93fa-af57-4bb4-a554-2b28b875e5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " The document appears to be a collection of proceedings from the 15th International Conference on Science and Innovative Engineering 2025, featuring various research papers on different topics. As such, there is no single main conclusion that can be drawn from the document as a whole. Each paper presents its own findings and conclusions, but there is no overarching conclusion that ties the entire document together.\n",
      "\n",
      "No source_documents returned with the result. You can still fetch them via retriever.get_relevant_documents(query).\n"
     ]
    }
   ],
   "source": [
    "# Robust extraction\n",
    "answer = None\n",
    "\n",
    "# common keys to check\n",
    "for key in (\"output_text\", \"result\", \"answer\", \"text\", \"output\"):\n",
    "    if key in result:\n",
    "        answer = result[key]\n",
    "        break\n",
    "\n",
    "# some versions embed the text under the chain's output key\n",
    "if not answer:\n",
    "    # print everything and then choose a likely field\n",
    "    # try the first string value\n",
    "    for v in result.values():\n",
    "        if isinstance(v, str) and len(v) > 0:\n",
    "            answer = v\n",
    "            break\n",
    "\n",
    "print(\"ANSWER:\\n\", answer)\n",
    "\n",
    "# If the chain returned source documents (sometimes under 'source_documents' or similar), print them\n",
    "if \"source_documents\" in result:\n",
    "    docs = result[\"source_documents\"]\n",
    "elif \"source_docs\" in result:\n",
    "    docs = result[\"source_docs\"]\n",
    "else:\n",
    "    # try to fetch returned sources from qa if available\n",
    "    try:\n",
    "        # Some RetrievalQA implementations support return_source_documents flag — might be in 'result'\n",
    "        docs = result.get(\"source_documents\", None)\n",
    "    except Exception:\n",
    "        docs = None\n",
    "\n",
    "if docs:\n",
    "    print(f\"\\nRetrieved {len(docs)} source documents:\")\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        src = getattr(d, \"metadata\", {}).get(\"source\", \"unknown\")\n",
    "        print(f\"\\n--- SOURCE {i} (source: {src}) ---\")\n",
    "        print(d.page_content[:800])\n",
    "else:\n",
    "    print(\"\\nNo source_documents returned with the result. You can still fetch them via retriever.get_relevant_documents(query).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fe8513b-9650-4b0e-8917-cc56e51aaff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa.run output:\n",
      " The document appears to be a collection of proceedings from the 15th International Conference on Science and Innovative Engineering 2025, featuring various research papers on different topics. There is no single main conclusion that can be drawn from the document as a whole, as each paper presents its own unique findings and contributions to its respective field.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    out = qa.run(query)\n",
    "    print(\"qa.run output:\\n\", out)\n",
    "except Exception as e:\n",
    "    print(\"qa.run failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba8c4f66-dc67-4f92-bdbf-2c375f306657",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "res = qa.invoke({\"query\": query})\n",
    "# then extract res['source_documents'] as above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d60b861-3948-4d8f-9c39-22056ad6cf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pages: 3\n",
      "Created chunks: 3\n",
      "Saved chunks JSONL to: faiss_index\\chunks.jsonl\n",
      "Built FAISS vectorstore with vectors: 3\n",
      "Saved FAISS index to folder: faiss_index\n",
      "Retriever ready with k=2. Done.\n"
     ]
    }
   ],
   "source": [
    "# === REBUILD FAISS WITH SMALLER CHUNKS (Notebook cell) ===\n",
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Config - change PDF_PATH if needed\n",
    "PDF_PATH = \"sample.pdf\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 80\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "SAVE_DIR = \"faiss_index\"\n",
    "CHUNKS_JSONL = os.path.join(SAVE_DIR, \"chunks.jsonl\")\n",
    "\n",
    "# Imports (make sure packages are installed)\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 1) Load PDF\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    raise FileNotFoundError(f\"Put your PDF at '{PDF_PATH}' or change the PDF_PATH variable.\")\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "docs = loader.load()\n",
    "print(\"Loaded pages:\", len(docs))\n",
    "\n",
    "# 2) Split into smaller chunks\n",
    "splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(\"Created chunks:\", len(chunks))\n",
    "\n",
    "# 3) Save chunks to JSONL (safe, portable)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "with open(CHUNKS_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for doc in chunks:\n",
    "        rec = {\"page_content\": doc.page_content, \"metadata\": getattr(doc, \"metadata\", {})}\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "print(\"Saved chunks JSONL to:\", CHUNKS_JSONL)\n",
    "\n",
    "# 4) Create embeddings and FAISS vectorstore\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "print(\"Built FAISS vectorstore with vectors:\", len(chunks))\n",
    "\n",
    "# 5) Persist FAISS index to disk\n",
    "vectorstore.save_local(SAVE_DIR)\n",
    "print(\"Saved FAISS index to folder:\", SAVE_DIR)\n",
    "\n",
    "# 6) Create retriever for immediate use (k=2 recommended)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "print(\"Retriever ready with k=2. Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20166819-6f08-4ba2-89e7-3c46cca9e269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1672 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw keys: ['query', 'result', 'source_documents']\n",
      "\n",
      "=== MAP-REDUCE ANSWER ===\n",
      " Not stated in the document.\n",
      "\n",
      "--- SOURCE 1 ---\n",
      "Proceedings of 15th International Conference on Science and Innovative Engineering 2025 \n",
      "April 26th - 27th, 2025 \n",
      "Prince Dr.K.Vasudevan college of Engineering and Technology, India  \n",
      "Manipal University College Malaysia, Melaka, Malaysia           ISBN 978-81-983498-5-9                                                                                                                       \n",
      " \n",
      " \n",
      "117. LMS PLATFORM USING GENERATIVE AI \n",
      " \n",
      "Ruben George Varghese \n",
      "Dharshan R E \n",
      "Harish Jayaram S S \n",
      "R Dheepthi \n",
      "Computer Science and Engineering \n",
      "Hindustan Institute of Technology and Science, \n",
      "Chennai, Tamil Nadu, India \n",
      " \n",
      "The \"LMS Platform Using Generative AI\" addresses the lack of personalized and engaging learning \n",
      "resources in traditional Learning Management Systems (LMS). By overcoming the limitation\n",
      "\n",
      "--- SOURCE 2 ---\n",
      "Proceedings of 15th International Conference on Science and Innovative Engineering 2025 \n",
      "April 26th - 27th, 2025 \n",
      "Prince Dr.K.Vasudevan college of Engineering and Technology, India  \n",
      "Manipal University College Malaysia, Melaka, Malaysia           ISBN 978-81-983498-5-9                                                                                                                       \n",
      " \n",
      " \n",
      " \n",
      "Sidon sets-SSs are subsets of real numbers possessing different totals for pair wise sums. Simon Sidon \n",
      "introduced it to settle a problem in harmonic analysis. Sidon, Erdos and Turan formed a group of three \n",
      "to popularize its study in 1934. In the field of combinatorics, the SSs are hotly pursued objects as these \n",
      "are employed in graph theory, coding theory, distributed computing etc. The task of deter\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from groq_remote_llm import GroqRemoteLLM\n",
    "\n",
    "# Create LLM instance (ensure env vars loaded)\n",
    "llm = GroqRemoteLLM()\n",
    "\n",
    "# Build a map-reduce RetrievalQA\n",
    "qa_map = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",   # map_reduce does per-chunk summaries then reduces\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True    # ask it to return sources too\n",
    ")\n",
    "\n",
    "query = \"Summarize the methodology section of the paper.\"\n",
    "res = qa_map.invoke({\"query\": query})\n",
    "print(\"Raw keys:\", list(res.keys()))\n",
    "# Extract answer robustly\n",
    "answer = res.get(\"output_text\") or res.get(\"result\") or next((v for v in res.values() if isinstance(v, str)), None)\n",
    "print(\"\\n=== MAP-REDUCE ANSWER ===\\n\", answer)\n",
    "\n",
    "# If sources returned, print them\n",
    "docs = res.get(\"source_documents\") or res.get(\"source_docs\")\n",
    "if docs:\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        print(f\"\\n--- SOURCE {i} ---\\n{d.page_content[:800]}\")\n",
    "else:\n",
    "    print(\"\\nNo source_documents in response — use retriever.get_relevant_documents(query) to inspect sources.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aec364c5-8d43-46be-a8bf-a8d88cea000e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(llm\u001b[38;5;241m=\u001b[39mllm, chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_reduce\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39mretriever, return_source_documents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the main conclusion of the paper?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m res \u001b[38;5;241m=\u001b[39m qa\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query})\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, res\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_text\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m res\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Inspect sources\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    164\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    170\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    171\u001b[0m         inputs,\n\u001b[0;32m    172\u001b[0m         outputs,\n\u001b[0;32m    173\u001b[0m         return_only_outputs,\n\u001b[0;32m    174\u001b[0m     )\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:159\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_documents_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    160\u001b[0m     input_documents\u001b[38;5;241m=\u001b[39mdocs,\n\u001b[0;32m    161\u001b[0m     question\u001b[38;5;241m=\u001b[39mquestion,\n\u001b[0;32m    162\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(),\n\u001b[0;32m    163\u001b[0m )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: answer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m: docs}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:193\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     emit_warning()\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:632\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    628\u001b[0m         _output_key\n\u001b[0;32m    629\u001b[0m     ]\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    633\u001b[0m         _output_key\n\u001b[0;32m    634\u001b[0m     ]\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    637\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    639\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    640\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:193\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     emit_warning()\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:410\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    403\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    408\u001b[0m }\n\u001b[1;32m--> 410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    411\u001b[0m     inputs,\n\u001b[0;32m    412\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    413\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    414\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    415\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    164\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    170\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    171\u001b[0m         inputs,\n\u001b[0;32m    172\u001b[0m         outputs,\n\u001b[0;32m    173\u001b[0m         return_only_outputs,\n\u001b[0;32m    174\u001b[0m     )\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:143\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    142\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 143\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    144\u001b[0m     docs,\n\u001b[0;32m    145\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(),\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys,\n\u001b[0;32m    147\u001b[0m )\n\u001b[0;32m    148\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:253\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[0;32m    248\u001b[0m result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    249\u001b[0m     Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[0;32m    252\u001b[0m ]\n\u001b[1;32m--> 253\u001b[0m result, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce_documents_chain\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    254\u001b[0m     result_docs,\n\u001b[0;32m    255\u001b[0m     token_max\u001b[38;5;241m=\u001b[39mtoken_max,\n\u001b[0;32m    256\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    258\u001b[0m )\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_intermediate_steps:\n\u001b[0;32m    260\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m [r[question_result_key] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m map_results]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\reduce.py:261\u001b[0m, in \u001b[0;36mReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Combine multiple documents recursively.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m    element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    255\u001b[0m result_docs, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collapse(\n\u001b[0;32m    256\u001b[0m     docs,\n\u001b[0;32m    257\u001b[0m     token_max\u001b[38;5;241m=\u001b[39mtoken_max,\n\u001b[0;32m    258\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    260\u001b[0m )\n\u001b[1;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_documents_chain\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    262\u001b[0m     docs\u001b[38;5;241m=\u001b[39mresult_docs,\n\u001b[0;32m    263\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    265\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:263\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs), {}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py:325\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:193\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     emit_warning()\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:410\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    403\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    408\u001b[0m }\n\u001b[1;32m--> 410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    411\u001b[0m     inputs,\n\u001b[0;32m    412\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    413\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    414\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    415\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    164\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    170\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    171\u001b[0m         inputs,\n\u001b[0;32m    172\u001b[0m         outputs,\n\u001b[0;32m    173\u001b[0m         return_only_outputs,\n\u001b[0;32m    174\u001b[0m     )\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py:127\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    124\u001b[0m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    125\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 127\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py:139\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    137\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    140\u001b[0m         prompts,\n\u001b[0;32m    141\u001b[0m         stop,\n\u001b[0;32m    142\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    144\u001b[0m     )\n\u001b[0;32m    145\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    146\u001b[0m     cast(\u001b[38;5;28mlist\u001b[39m, prompts),\n\u001b[0;32m    147\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks},\n\u001b[0;32m    148\u001b[0m )\n\u001b[0;32m    149\u001b[0m generations: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Generation]] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:789\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    787\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    788\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 789\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1000\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    987\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    988\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    998\u001b[0m         )\n\u001b[0;32m    999\u001b[0m     ]\n\u001b[1;32m-> 1000\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m   1001\u001b[0m         prompts,\n\u001b[0;32m   1002\u001b[0m         stop,\n\u001b[0;32m   1003\u001b[0m         run_managers,\n\u001b[0;32m   1004\u001b[0m         new_arg_supported\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[0;32m   1005\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1006\u001b[0m     )\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1008\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1009\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m   1010\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[0;32m   1018\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:815\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    806\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    812\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 815\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    816\u001b[0m                 prompts,\n\u001b[0;32m    817\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    818\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    819\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    820\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    821\u001b[0m             )\n\u001b[0;32m    822\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    823\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    824\u001b[0m         )\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1580\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1575\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1577\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1578\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1579\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m-> 1580\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1581\u001b[0m     )\n\u001b[0;32m   1582\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain_env\\streamit_proj\\groq_remote_llm.py:37\u001b[0m, in \u001b[0;36mGroqRemoteLLM._call\u001b[1;34m(self, prompt, stop)\u001b[0m\n\u001b[0;32m     35\u001b[0m resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url, json\u001b[38;5;241m=\u001b[39mpayload, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[0;32m     36\u001b[0m doc \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'choices'"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from groq_remote_llm import GroqRemoteLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "emb = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", emb, allow_dangerous_deserialization=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})  # k=2 for precision\n",
    "\n",
    "llm = GroqRemoteLLM()\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "query = \"What is the main conclusion of the paper?\"\n",
    "res = qa.invoke({\"query\": query})\n",
    "print(\"Answer:\", res.get(\"output_text\") or res.get(\"result\"))\n",
    "\n",
    "# Inspect sources\n",
    "for i, d in enumerate(res.get(\"source_documents\", []), 1):\n",
    "    print(f\"\\n--- SOURCE {i} ---\\n{d.page_content[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051187bd-024c-49ee-8016-7fbdfd3c17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from groq_remote_llm import GroqRemoteLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "emb = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", emb, allow_dangerous_deserialization=True)  # only if you saved via save_local\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "llm = GroqRemoteLLM()\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=retriever, return_source_documents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f8fad2c-ff55-4b97-9a25-dd97a7339888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arul\\AppData\\Local\\Temp\\ipykernel_2588\\2500583640.py:17: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n",
      "C:\\Users\\Arul\\AppData\\Local\\Temp\\ipykernel_2588\\2500583640.py:35: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  rows.append([datetime.datetime.utcnow().isoformat(), q, answer, previews])\n",
      "C:\\Users\\Arul\\AppData\\Local\\Temp\\ipykernel_2588\\2500583640.py:35: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  rows.append([datetime.datetime.utcnow().isoformat(), q, answer, previews])\n",
      "C:\\Users\\Arul\\AppData\\Local\\Temp\\ipykernel_2588\\2500583640.py:37: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  rows.append([datetime.datetime.utcnow().isoformat(), q, f\"ERROR: {e}\", \"\"])\n",
      "C:\\Users\\Arul\\AppData\\Local\\Temp\\ipykernel_2588\\2500583640.py:37: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  rows.append([datetime.datetime.utcnow().isoformat(), q, f\"ERROR: {e}\", \"\"])\n",
      "C:\\Users\\Arul\\AppData\\Local\\Temp\\ipykernel_2588\\2500583640.py:37: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  rows.append([datetime.datetime.utcnow().isoformat(), q, f\"ERROR: {e}\", \"\"])\n",
      "Running queries: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:11<00:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged 5 rows to qa_audit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === QA AUDIT / EVALUATION SCRIPT ===\n",
    "import csv, datetime, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Edit or add queries you want to test\n",
    "queries = [\n",
    "    \"What is the main conclusion of the document?\",\n",
    "    \"What methods were used in the study?\",\n",
    "    \"Who are the authors of the paper?\",\n",
    "    \"What dataset did the authors use for evaluation?\",\n",
    "    \"List any limitations mentioned.\"\n",
    "]\n",
    "\n",
    "OUT_CSV = \"qa_audit.csv\"\n",
    "\n",
    "def get_top_chunk_previews(question, k=3, chars=300):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    previews = [d.page_content[:chars].replace(\"\\n\",\" \") for d in docs[:k]]\n",
    "    return \" ||| \".join(previews)\n",
    "\n",
    "# Run and log\n",
    "rows = []\n",
    "for q in tqdm(queries, desc=\"Running queries\"):\n",
    "    try:\n",
    "        # Use invoke for new API (returns dict) when available\n",
    "        try:\n",
    "            res = qa.invoke({\"query\": q})\n",
    "            # extract string answer robustly\n",
    "            answer = res.get(\"output_text\") or res.get(\"result\") or next((v for v in res.values() if isinstance(v, str)), \"\")\n",
    "        except Exception:\n",
    "            # fallback to legacy run()\n",
    "            answer = qa.run(q)\n",
    "            res = {}\n",
    "        previews = get_top_chunk_previews(q, k=3, chars=300)\n",
    "        rows.append([datetime.datetime.utcnow().isoformat(), q, answer, previews])\n",
    "    except Exception as e:\n",
    "        rows.append([datetime.datetime.utcnow().isoformat(), q, f\"ERROR: {e}\", \"\"])\n",
    "\n",
    "# Append to CSV\n",
    "os.makedirs(\".\", exist_ok=True)\n",
    "write_header = not os.path.exists(OUT_CSV)\n",
    "with open(OUT_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    if write_header:\n",
    "        writer.writerow([\"timestamp_utc\", \"question\", \"answer\", \"top_chunk_previews\"])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Logged {len(rows)} rows to {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9013bcfd-f70d-404a-8ef0-1f3554c91676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 'qa_audit.csv' with encoding utf-8 (rows: 5)\n",
      "Failed to read 'gold_answers.csv' with encoding utf-8, trying next encoding...\n",
      "Read 'gold_answers.csv' with encoding latin-1 (rows: 6)\n",
      "Wrote evaluation results to 'qa_eval_results.csv' (rows: 5)\n",
      "\n",
      "Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_utc</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>gold_answer</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>fuzzy_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-02T14:41:20.908400</td>\n",
       "      <td>What is the main conclusion of the document?</td>\n",
       "      <td>Not stated in the document.</td>\n",
       "      <td>The proposed Hybrid GRU-CNN classification str...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-02T14:41:24.628699</td>\n",
       "      <td>What methods were used in the study?</td>\n",
       "      <td>The methods used in the study include:\\n\\n1. F...</td>\n",
       "      <td>A hybrid GRU (Gated Recurrent Unit) + CNN (Con...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-10-02T14:41:27.594518</td>\n",
       "      <td>Who are the authors of the paper?</td>\n",
       "      <td>ERROR: `run` not supported when there is not e...</td>\n",
       "      <td>Arul Selvam P (Assistant Professor, Dept. of C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-10-02T14:41:28.406473</td>\n",
       "      <td>What dataset did the authors use for evaluation?</td>\n",
       "      <td>ERROR: `run` not supported when there is not e...</td>\n",
       "      <td>The Insider Threat Test Dataset developed by t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-10-02T14:41:29.283210</td>\n",
       "      <td>List any limitations mentioned.</td>\n",
       "      <td>ERROR: `run` not supported when there is not e...</td>\n",
       "      <td>Existing research has not taken into account t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp_utc  \\\n",
       "0  2025-10-02T14:41:20.908400   \n",
       "1  2025-10-02T14:41:24.628699   \n",
       "2  2025-10-02T14:41:27.594518   \n",
       "3  2025-10-02T14:41:28.406473   \n",
       "4  2025-10-02T14:41:29.283210   \n",
       "\n",
       "                                           question  \\\n",
       "0      What is the main conclusion of the document?   \n",
       "1              What methods were used in the study?   \n",
       "2                 Who are the authors of the paper?   \n",
       "3  What dataset did the authors use for evaluation?   \n",
       "4                   List any limitations mentioned.   \n",
       "\n",
       "                                              answer  \\\n",
       "0                        Not stated in the document.   \n",
       "1  The methods used in the study include:\\n\\n1. F...   \n",
       "2  ERROR: `run` not supported when there is not e...   \n",
       "3  ERROR: `run` not supported when there is not e...   \n",
       "4  ERROR: `run` not supported when there is not e...   \n",
       "\n",
       "                                         gold_answer  exact_match  fuzzy_ratio  \n",
       "0  The proposed Hybrid GRU-CNN classification str...            0       0.0074  \n",
       "1  A hybrid GRU (Gated Recurrent Unit) + CNN (Con...            0       0.1216  \n",
       "2  Arul Selvam P (Assistant Professor, Dept. of C...            0       0.1953  \n",
       "3  The Insider Threat Test Dataset developed by t...            0       0.2648  \n",
       "4  Existing research has not taken into account t...            0       0.1825  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact match rate: 0.000  (0%)\n",
      "Average fuzzy ratio: 0.154\n"
     ]
    }
   ],
   "source": [
    "# Robust evaluation cell: reads CSVs with encoding fallback, scores QA outputs vs gold answers.\n",
    "import csv, difflib, os\n",
    "from statistics import mean\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "QA_FILE = \"qa_audit.csv\"\n",
    "GOLD_FILE = \"gold_answers.csv\"\n",
    "OUT_FILE = \"qa_eval_results.csv\"\n",
    "\n",
    "def read_csv_with_fallback(path):\n",
    "    \"\"\"\n",
    "    Try utf-8, then latin-1. Return list of dict rows (csv.DictReader).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    for enc in (\"utf-8\", \"latin-1\"):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                rows = [r for r in reader]\n",
    "            print(f\"Read '{path}' with encoding {enc} (rows: {len(rows)})\")\n",
    "            return rows\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to read '{path}' with encoding {enc}, trying next encoding...\")\n",
    "    # final fallback: latin-1 with replace to avoid crashes\n",
    "    with open(path, \"r\", encoding=\"latin-1\", errors=\"replace\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        rows = [r for r in reader]\n",
    "    print(f\"Read '{path}' with encoding latin-1 (replace errors) (rows: {len(rows)})\")\n",
    "    return rows\n",
    "\n",
    "# 1) Ensure QA file exists\n",
    "if not os.path.exists(QA_FILE):\n",
    "    raise FileNotFoundError(f\"Expected QA audit file '{QA_FILE}' not found. Run audit step first.\")\n",
    "\n",
    "# 2) If gold file missing, create template from QA questions and exit (so user fills gold answers)\n",
    "if not os.path.exists(GOLD_FILE):\n",
    "    print(f\"'{GOLD_FILE}' not found. Creating template from audit file questions.\")\n",
    "    qa_rows = read_csv_with_fallback(QA_FILE)\n",
    "    questions = []\n",
    "    for r in qa_rows:\n",
    "        q = r.get(\"question\") or r.get(\"Question\") or r.get(\"question_text\")\n",
    "        if q:\n",
    "            questions.append(q.strip())\n",
    "    questions = sorted(set(questions))\n",
    "    with open(GOLD_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"question\",\"gold_answer\"])\n",
    "        for q in questions:\n",
    "            w.writerow([q, \"\"])  # user fills gold answers\n",
    "    print(f\"Wrote template '{GOLD_FILE}'. Please open it, fill the 'gold_answer' column, save, and re-run this cell.\")\n",
    "    raise SystemExit(\"Fill gold_answers.csv with expected answers, then re-run this cell.\")\n",
    "\n",
    "# 3) Read files (with fallback)\n",
    "qa_rows = read_csv_with_fallback(QA_FILE)\n",
    "gold_rows = read_csv_with_fallback(GOLD_FILE)\n",
    "\n",
    "# 4) Build gold dict (strip keys) and QA list\n",
    "gold = {}\n",
    "for r in gold_rows:\n",
    "    q = (r.get(\"question\") or r.get(\"Question\") or \"\").strip()\n",
    "    g = (r.get(\"gold_answer\") or r.get(\"gold\") or \"\").strip()\n",
    "    if q:\n",
    "        gold[q] = g\n",
    "\n",
    "# 5) Score each QA row (handle various column names)\n",
    "scored = []\n",
    "for r in qa_rows:\n",
    "    q = (r.get(\"question\") or r.get(\"Question\") or r.get(\"question_text\") or \"\").strip()\n",
    "    ans = (r.get(\"answer\") or r.get(\"Answer\") or r.get(\"answer_text\") or \"\").strip()\n",
    "    if not q:\n",
    "        continue\n",
    "    gold_ans = gold.get(q, \"\").strip()\n",
    "    exact = 1 if (gold_ans and ans.lower() == gold_ans.lower()) else 0\n",
    "    ratio = difflib.SequenceMatcher(None, ans.lower(), gold_ans.lower()).ratio() if gold_ans else 0.0\n",
    "    scored.append({\n",
    "        \"timestamp_utc\": r.get(\"timestamp_utc\") or datetime.utcnow().isoformat(),\n",
    "        \"question\": q,\n",
    "        \"answer\": ans,\n",
    "        \"gold_answer\": gold_ans,\n",
    "        \"exact_match\": exact,\n",
    "        \"fuzzy_ratio\": round(ratio, 4)\n",
    "    })\n",
    "\n",
    "# 6) Save results to OUT_FILE\n",
    "if scored:\n",
    "    keys = [\"timestamp_utc\",\"question\",\"answer\",\"gold_answer\",\"exact_match\",\"fuzzy_ratio\"]\n",
    "    with open(OUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(scored)\n",
    "    print(f\"Wrote evaluation results to '{OUT_FILE}' (rows: {len(scored)})\")\n",
    "else:\n",
    "    print(\"No QA rows to score. Check your QA audit file.\")\n",
    "\n",
    "# 7) Print summary and preview\n",
    "df = pd.read_csv(OUT_FILE)\n",
    "print(\"\\nPreview:\")\n",
    "display(df.head(10))\n",
    "\n",
    "if len(df):\n",
    "    exact_rate = df[\"exact_match\"].mean()\n",
    "    avg_fuzzy = df[\"fuzzy_ratio\"].mean()\n",
    "    print(f\"\\nExact match rate: {exact_rate:.3f}  ({int(exact_rate*100)}%)\")\n",
    "    print(f\"Average fuzzy ratio: {avg_fuzzy:.3f}\")\n",
    "else:\n",
    "    print(\"No scored rows found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b643bb6-45db-4b5d-9b04-8b6c4635a7d9",
   "metadata": {},
   "source": [
    "**Notes & troubleshooting**\n",
    "\n",
    "- If the Groq LLM call fails, check your `.env` and run the connectivity `requests.post` test shown earlier.\n",
    "- If `SentenceTransformer` import errors occur, set `os.environ['TRANSFORMERS_NO_TF']='1'` before importing or use a clean conda env.\n",
    "- To reuse the saved FAISS index in a Streamlit app, use `FAISS.load_local` at app startup.\n",
    "\n",
    "Enjoy — run the cells sequentially from top to bottom. If you want, I can also provide this notebook as a downloadable file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a27e69c-6aae-43ef-9ac9-875c1eca8694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pages: 3\n",
      "Created chunks: 3\n",
      "Saved chunks JSONL to: faiss_index\\chunks.jsonl\n",
      "Built and saved FAISS index in: faiss_index\n",
      "Retriever ready with k = 3\n",
      "Created StrictGroqLLM with model: llama-3.3-70b-versatile\n",
      "Created RetrievalQA (refine).\n",
      "\n",
      "--- Running test query ---\n",
      "Query: What is the main conclusion of the document?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arul\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw result keys: ['query', 'result', 'source_documents']\n",
      "\n",
      "=== ANSWER ===\n",
      " Not stated in the document. \n",
      "\n",
      "The new context provided still appears to be a collection of abstracts or summaries of different research papers or presentations, each with its own unique topic and conclusion. The first part discusses Sidon sets and their applications in graph theory, while the second part describes a project for an IoT-enabled smart wheelchair. There is no overarching main conclusion that encompasses the entire document.\n",
      "\n",
      "Returned 3 source documents (first 3 shown):\n",
      "\n",
      "--- SOURCE 1 (source: sample.pdf) ---\n",
      "\n",
      "Proceedings of 15th International Conference on Science and Innovative Engineering 2025 \n",
      "April 26th - 27th, 2025 \n",
      "Prince Dr.K.Vasudevan college of Engineering and Technology, India  \n",
      "Manipal University College Malaysia, Melaka, Malaysia           ISBN 978-81-983498-5-9                                                                                                                       \n",
      " \n",
      " \n",
      "Jerusalem College of Engineering, Chennai-600100. \n",
      " \n",
      "In the era of big data, cloud storage services have become a fundamental tool for managing and storing \n",
      "large volumes of data. However, with the increase in data storage, the challenge of data deduplication—\n",
      "eliminating duplicate copies of repeating data—becomes critical to optimize storage space and reduce \n",
      "costs. ABATE (Deduplication Using Blockchain\n",
      "\n",
      "--- SOURCE 2 (source: sample.pdf) ---\n",
      "\n",
      "Proceedings of 15th International Conference on Science and Innovative Engineering 2025 \n",
      "April 26th - 27th, 2025 \n",
      "Prince Dr.K.Vasudevan college of Engineering and Technology, India  \n",
      "Manipal University College Malaysia, Melaka, Malaysia           ISBN 978-81-983498-5-9                                                                                                                       \n",
      " \n",
      " \n",
      "117. LMS PLATFORM USING GENERATIVE AI \n",
      " \n",
      "Ruben George Varghese \n",
      "Dharshan R E \n",
      "Harish Jayaram S S \n",
      "R Dheepthi \n",
      "Computer Science and Engineering \n",
      "Hindustan Institute of Technology and Science, \n",
      "Chennai, Tamil Nadu, India \n",
      " \n",
      "The \"LMS Platform Using Generative AI\" addresses the lack of personalized and engaging learning \n",
      "resources in traditional Learning Management Systems (LMS). By overcoming the limitation\n",
      "\n",
      "--- SOURCE 3 (source: sample.pdf) ---\n",
      "\n",
      "Proceedings of 15th International Conference on Science and Innovative Engineering 2025 \n",
      "April 26th - 27th, 2025 \n",
      "Prince Dr.K.Vasudevan college of Engineering and Technology, India  \n",
      "Manipal University College Malaysia, Melaka, Malaysia           ISBN 978-81-983498-5-9                                                                                                                       \n",
      " \n",
      " \n",
      " \n",
      "Sidon sets-SSs are subsets of real numbers possessing different totals for pair wise sums. Simon Sidon \n",
      "introduced it to settle a problem in harmonic analysis. Sidon, Erdos and Turan formed a group of three \n",
      "to popularize its study in 1934. In the field of combinatorics, the SSs are hotly pursued objects as these \n",
      "are employed in graph theory, coding theory, distributed computing etc. The task of deter\n",
      "\n",
      "--- Grounding check for tokens from gold answer ---\n",
      "Token 'GRU': FOUND\n",
      "Token 'CNN': FOUND\n",
      "Token 'GRU-CNN': FOUND\n",
      "Token '98.43': FOUND\n",
      "Token '97.12': FOUND\n",
      "Token 'insider': FOUND\n",
      "Token 'classification': FOUND\n",
      "Token 'accuracy': NOT FOUND\n",
      "\n",
      "Done. If the answer looks good and tokens are FOUND, grounding improved.\n"
     ]
    }
   ],
   "source": [
    "# Single cell: rebuild chunks (400/100), rebuild FAISS, create strict Groq LLM, use refine chain, test query + grounding check.\n",
    "\n",
    "import os, json, traceback\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    # ----------------- CONFIG -----------------\n",
    "    PDF_PATH = \"sample.pdf\"   # change if needed\n",
    "    SAVE_DIR = \"faiss_index\"\n",
    "    CHUNK_SIZE = 400\n",
    "    CHUNK_OVERLAP = 100\n",
    "    EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "    RETRIEVER_K = 3\n",
    "    TEST_QUERY = \"What is the main conclusion of the document?\"\n",
    "    # tokens to check for grounding (extract from your gold answer)\n",
    "    GROUND_TOKENS = [\"GRU\", \"CNN\", \"GRU-CNN\", \"98.43\", \"97.12\", \"insider\", \"classification\", \"accuracy\"]\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # 1) imports (do here so errors show if libs missing)\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    from langchain.text_splitter import CharacterTextSplitter\n",
    "    from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "    from langchain.vectorstores import FAISS\n",
    "    from langchain.schema import Document\n",
    "\n",
    "    # 2) load PDF and re-split (smaller chunks)\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        raise FileNotFoundError(f\"PDF not found: {PDF_PATH}. Place it in the working folder or change PDF_PATH.\")\n",
    "    loader = PyPDFLoader(PDF_PATH)\n",
    "    pages = loader.load()\n",
    "    print(\"Loaded pages:\", len(pages))\n",
    "\n",
    "    splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    chunks = splitter.split_documents(pages)\n",
    "    print(\"Created chunks:\", len(chunks))\n",
    "\n",
    "    # 3) save raw chunks JSONL (portable, avoids pickle reliance)\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    chunks_file = os.path.join(SAVE_DIR, \"chunks.jsonl\")\n",
    "    with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for d in chunks:\n",
    "            rec = {\"page_content\": d.page_content, \"metadata\": getattr(d, \"metadata\", {})}\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "    print(\"Saved chunks JSONL to:\", chunks_file)\n",
    "\n",
    "    # 4) create embeddings and FAISS\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    vectorstore.save_local(SAVE_DIR)\n",
    "    print(\"Built and saved FAISS index in:\", SAVE_DIR)\n",
    "\n",
    "    # 5) reload fresh (simulate fresh session)\n",
    "    emb = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "    vectorstore = FAISS.load_local(SAVE_DIR, emb, allow_dangerous_deserialization=True)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": RETRIEVER_K})\n",
    "    print(\"Retriever ready with k =\", RETRIEVER_K)\n",
    "\n",
    "    # 6) Create a custom Groq LLM with strict system prompt & temp=0.0\n",
    "    #    We define it inline so you don't need to edit groq_remote_llm.py on disk.\n",
    "    try:\n",
    "        from langchain.llms.base import LLM\n",
    "    except Exception:\n",
    "        class LLM: pass\n",
    "\n",
    "    from pydantic import BaseModel, Field\n",
    "    import requests\n",
    "    from typing import Optional, List, Mapping, Any\n",
    "\n",
    "    class StrictGroqLLM(LLM, BaseModel):\n",
    "        api_url: str = Field(default_factory=lambda: os.getenv(\"GROQ_API_URL\"))\n",
    "        api_key: str = Field(default_factory=lambda: os.getenv(\"GROQ_API_KEY\"))\n",
    "        model: str   = Field(default_factory=lambda: os.getenv(\"GROQ_MODEL\", \"llama-3.3-70b-versatile\"))\n",
    "        timeout: int = 60\n",
    "        temperature: float = 0.0\n",
    "        max_tokens: int = 256\n",
    "\n",
    "        class Config:\n",
    "            arbitrary_types_allowed = True\n",
    "            allow_population_by_field_name = True\n",
    "\n",
    "        @property\n",
    "        def _llm_type(self) -> str:\n",
    "            return \"strict-groq-llm\"\n",
    "\n",
    "        @property\n",
    "        def _identifying_params(self) -> Mapping[str, Any]:\n",
    "            return {\"model\": self.model, \"url\": self.api_url}\n",
    "\n",
    "        def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "            if not self.api_url:\n",
    "                raise RuntimeError(\"GROQ_API_URL not set in environment.\")\n",
    "            if not self.api_key:\n",
    "                raise RuntimeError(\"GROQ_API_KEY not set in environment.\")\n",
    "            headers = {\"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\"}\n",
    "            # Strict system prompt: answer only from provided context, else say Not stated...\n",
    "            \n",
    "            system_msg = (\n",
    "                \"You are an information extraction assistant. \"\n",
    "                \"Carefully read the provided document chunks and answer factually. \"\n",
    "                \"If the answer is explicitly mentioned (even partially), extract it and restate clearly. \"\n",
    "                \"Include numbers, model names, and datasets if they appear. \"\n",
    "                \"Only if the information is completely absent in the context, respond exactly: 'Not stated in the document.'\"\n",
    "            )\n",
    "\n",
    "            \n",
    "            payload = {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": [\n",
    "                    {\"role\":\"system\", \"content\": system_msg},\n",
    "                    {\"role\":\"user\", \"content\": prompt},\n",
    "                ],\n",
    "                \"temperature\": float(self.temperature),\n",
    "                \"max_tokens\": int(self.max_tokens),\n",
    "            }\n",
    "            resp = requests.post(self.api_url, json=payload, headers=headers, timeout=self.timeout)\n",
    "            if not resp.ok:\n",
    "                raise RuntimeError(f\"Groq API error {resp.status_code}: {resp.text[:1000]}\")\n",
    "            doc = resp.json()\n",
    "            # robust extraction\n",
    "            if isinstance(doc, dict):\n",
    "                if \"choices\" in doc and isinstance(doc[\"choices\"], list) and doc[\"choices\"]:\n",
    "                    first = doc[\"choices\"][0]\n",
    "                    if isinstance(first, dict):\n",
    "                        if \"message\" in first and isinstance(first[\"message\"], dict):\n",
    "                            return first[\"message\"].get(\"content\")\n",
    "                        if \"text\" in first:\n",
    "                            return first.get(\"text\")\n",
    "                for key in (\"text\",\"output_text\",\"result\",\"output\"):\n",
    "                    if key in doc:\n",
    "                        v = doc[key]\n",
    "                        if isinstance(v, str):\n",
    "                            return v\n",
    "                        return str(v)\n",
    "            return str(doc)\n",
    "\n",
    "    # instantiate it\n",
    "    llm = StrictGroqLLM()\n",
    "    print(\"Created StrictGroqLLM with model:\", llm.model)\n",
    "\n",
    "    # 7) Create RetrievalQA with chain_type = 'refine' and return_source_documents\n",
    "    from langchain.chains import RetrievalQA\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=True)\n",
    "    print(\"Created RetrievalQA (refine).\")\n",
    "\n",
    "    # 8) Run the test query using invoke() (new API)\n",
    "    print(\"\\n--- Running test query ---\\nQuery:\", TEST_QUERY)\n",
    "    res = qa.invoke({\"query\": TEST_QUERY})\n",
    "    print(\"\\nRaw result keys:\", list(res.keys()))\n",
    "\n",
    "    # Extract answer robustly\n",
    "    answer = res.get(\"output_text\") or res.get(\"result\") or next((v for v in res.values() if isinstance(v, str)), None)\n",
    "    print(\"\\n=== ANSWER ===\\n\", answer)\n",
    "\n",
    "    # Show returned sources when available\n",
    "    docs = res.get(\"source_documents\") or res.get(\"source_docs\")\n",
    "    if docs:\n",
    "        print(f\"\\nReturned {len(docs)} source documents (first 3 shown):\")\n",
    "        for i, d in enumerate(docs[:3], 1):\n",
    "            src = getattr(d, \"metadata\", {}).get(\"source\", \"unknown\")\n",
    "            print(f\"\\n--- SOURCE {i} (source: {src}) ---\\n\")\n",
    "            print(d.page_content[:800])\n",
    "    else:\n",
    "        print(\"\\nNo source_documents included in response; retrieving top-k via retriever for inspection.\")\n",
    "        docs = retriever.get_relevant_documents(TEST_QUERY)\n",
    "        for i, d in enumerate(docs[:3], 1):\n",
    "            print(f\"\\n--- Retrieved chunk {i} ---\\n\")\n",
    "            print(d.page_content[:800])\n",
    "\n",
    "    # 9) Grounding check: look for target tokens inside the returned docs\n",
    "    combined_text = \" \".join([d.page_content for d in (docs or [])]).lower()\n",
    "    print(\"\\n--- Grounding check for tokens from gold answer ---\")\n",
    "    for t in GROUND_TOKENS:\n",
    "        found = (t.lower() in combined_text)\n",
    "        print(f\"Token '{t}':\", \"FOUND\" if found else \"NOT FOUND\")\n",
    "\n",
    "    print(\"\\nDone. If the answer looks good and tokens are FOUND, grounding improved.\")\n",
    "except Exception as e:\n",
    "    print(\"ERROR during rebuild+test:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d34acb58-d65b-48e0-86c1-e80ecd2996f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RetrievalQA built with map_reduce + custom prompt.\n",
      "\n",
      "=== ANSWER ===\n",
      " There are two separate main conclusions presented in the document: \n",
      "1. The \"LMS Platform Using Generative AI\" enhances learning effectiveness by integrating generative AI for personalized content.\n",
      "2. The Hybrid GRU-CNN approach achieved high accuracy (98.43% in training and 97.12% in testing) and low false positive rates (0.17% in training and 2.88% in testing) for insider attack classification in cloud networks using the Insider Threat Test Dataset.\n",
      "\n",
      "Returned 3 sources (showing first 2):\n",
      "\n",
      "--- SOURCE 1 ---\n",
      "Proceedings of 15th International Conference on Science and Innovative Engineering 2025 \n",
      "April 26th - 27th, 2025 \n",
      "Prince Dr.K.Vasudevan college of Engineering and Technology, India  \n",
      "Manipal University College Malaysia, Melaka, Malaysia           ISBN 978-81-983498-5-9                                                                                                                       \n",
      " \n",
      " \n",
      "Jerusalem College of Engineering, Chennai-600100. \n",
      " \n",
      "In the era of big data, cloud storage services have be\n",
      "\n",
      "--- SOURCE 2 ---\n",
      "Proceedings of 15th International Conference on Science and Innovative Engineering 2025 \n",
      "April 26th - 27th, 2025 \n",
      "Prince Dr.K.Vasudevan college of Engineering and Technology, India  \n",
      "Manipal University College Malaysia, Melaka, Malaysia           ISBN 978-81-983498-5-9                                                                                                                       \n",
      " \n",
      " \n",
      "117. LMS PLATFORM USING GENERATIVE AI \n",
      " \n",
      "Ruben George Varghese \n",
      "Dharshan R E \n",
      "Harish Jayaram S S \n",
      "R Dheepth\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# ---- Custom Q&A Prompt ----\n",
    "prompt_template = \"\"\"\n",
    "You are given the following document context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Task: Extract the factual answer as clearly and concisely as possible.\n",
    "- Include technical terms (e.g., GRU, CNN), dataset names, and numbers (accuracy, percentages).\n",
    "- If info is not in the context, respond: \"Not stated in the document.\"\n",
    "- Do not add extra info.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# ---- Build RetrievalQA chain ----\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"question_prompt\": QA_PROMPT},  # ✅ use question_prompt, not prompt\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"✅ RetrievalQA built with map_reduce + custom prompt.\")\n",
    "\n",
    "# ---- Run test query ----\n",
    "query = \"What is the main conclusion of the document?\"\n",
    "res = qa.invoke({\"query\": query})\n",
    "\n",
    "print(\"\\n=== ANSWER ===\\n\", res[\"result\"])\n",
    "\n",
    "if \"source_documents\" in res:\n",
    "    print(f\"\\nReturned {len(res['source_documents'])} sources (showing first 2):\")\n",
    "    for i, d in enumerate(res[\"source_documents\"][:2], 1):\n",
    "        print(f\"\\n--- SOURCE {i} ---\\n{d.page_content[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf03046-d0aa-4f0e-9cd6-8d46745c58a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
